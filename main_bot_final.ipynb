{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVUvCwohkxXJ",
    "outputId": "cd6ed9a7-129e-49fd-806f-e1488a016675",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/linux/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/linux/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "from dateutil import parser\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import pickle\n",
    "import telebot\n",
    "from telebot import types\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4v1_SjNdsuad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_soup(link: str):\n",
    "    contents = requests.get(link).text\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_google_research_publications():\n",
    "    publications = requests.get(\n",
    "        \"https://research.google/static/data/publications-40ebbba8cb2c591bd21b031bcd13e1d58f8a60e2055cda8775f8600d031f884a.json\")\n",
    "    publications = publications.json()[\"publications\"]\n",
    "    meta_publications = []\n",
    "    for publication in publications:\n",
    "        venue_text = BeautifulSoup(publication[\"venue_html\"], 'lxml').text\n",
    "        try:\n",
    "            publication_data = {\n",
    "                \"bibtex\": publication[\"bibtex\"],\n",
    "                \"title\": publication[\"title\"],\n",
    "                \"venue_text\": venue_text,\n",
    "                \"time_release\": str(publication[\"year\"]),\n",
    "                \"tag_pks\": publication[\"tag_pks\"],\n",
    "                \"abstract\": publication[\"abstract\"],\n",
    "                \"link\": f\"https://research.google/pubs/{publication['filename_html'].split('.')[0]}\"\n",
    "            }\n",
    "            meta_publications.append(publication_data)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return meta_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deepmind_research_publications():\n",
    "    publications = requests.get(\"https://www.deepmind.com/publications/rss.xml\")\n",
    "    publications = BeautifulSoup(publications.content, 'lxml')\n",
    "    publications = publications.find_all(\"item\")\n",
    "    \n",
    "    meta_publications = []\n",
    "    for publication in publications:\n",
    "        publication_data = {\n",
    "            \"title\": publication.find(\"title\").text,\n",
    "            \"link\": publication.find(\"guid\").text,\n",
    "            \"time_release\": publication.find(\"pubdate\").text,\n",
    "            \"description\": publication.find(\"description\").text,\n",
    "        }\n",
    "        meta_publications.append(publication_data)\n",
    "    return meta_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9PnePLHXtEqH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_yandex_research_publications():\n",
    "    publications = requests.get(\n",
    "        \"https://research.yandex.com/api/publications-filtered?pagination[start]=0&pagination[limit]=1000\")\n",
    "    publications_json = publications.json()[\"data\"]\n",
    "\n",
    "    meta_publications = []\n",
    "    for publication in publications_json:\n",
    "        authors, organizations = [], set()\n",
    "        for author in publication[\"authors\"]:\n",
    "            authors.append(author[\"name\"])\n",
    "            for key in author:\n",
    "                if key.startswith(\"captionString\"):\n",
    "                    if author[key]:\n",
    "                        organizations.add(author[key])\n",
    "\n",
    "        organizations = Counter(organizations).most_common()\n",
    "        organizations = [i[0] for i in organizations]\n",
    "        research_areas = [area[\"title\"] for area in publication[\"researchAreas\"]]\n",
    "\n",
    "        publication_data = {\n",
    "            \"title\": publication[\"title\"],\n",
    "            \"abstract\": publication[\"abstract\"],\n",
    "            \"time_release\": publication[\"publicationDate\"],\n",
    "            \"slug\": publication[\"slug\"],\n",
    "            \"downloadUrl\": publication[\"downloadUrl\"],\n",
    "            \"researchAreas\": research_areas,\n",
    "            \"authors\": authors,\n",
    "            \"organizations\": organizations,\n",
    "            \"venue\": publication[\"venue\"],\n",
    "            \"link\": f\"https://research.yandex.com/publications/{publication['slug']}\"\n",
    "        }\n",
    "        meta_publications.append(publication_data)\n",
    "\n",
    "    return meta_publications\n",
    "\n",
    "\n",
    "def get_yandex_research_posts():\n",
    "    posts = requests.get(\n",
    "        \"https://research.yandex.com/api/posts-filtered?pagination[start]=0&pagination[limit]=1000\")\n",
    "    posts_json = posts.json()[\"data\"]\n",
    "\n",
    "    meta_posts = []\n",
    "    for publication in posts_json:\n",
    "        research_areas = [area[\"title\"] for area in publication[\"researchAreas\"]]\n",
    "\n",
    "        publication_data = {\n",
    "            \"title\": publication[\"title\"],\n",
    "            \"time_release\": publication[\"publicationDate\"],\n",
    "            \"slug\": publication[\"slug\"],\n",
    "            \"researchAreas\": research_areas,\n",
    "            \"type\": publication[\"type\"],\n",
    "            \"seoDescription\": publication[\"seoDescription\"],\n",
    "            \"link\": f\"https://research.yandex.com/blog/{publication['slug']}\"\n",
    "        }\n",
    "        meta_posts.append(publication_data)\n",
    "\n",
    "    return meta_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xw0xKs2WFR5U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------- huggingface\n",
    "\n",
    "def get_article_link_hf(soup):\n",
    "    prettify_html = soup.prettify()\n",
    "    start, stop = re.search(r'href=\".*\"', prettify_html).span()\n",
    "    return f\"https://huggingface.co{prettify_html[start:stop][6:-1]}\"\n",
    "\n",
    "\n",
    "def soup_to_embed_hf(soup):\n",
    "    results = []\n",
    "    for element in soup.find(\"div\", {\"class\": \"grid grid-cols-1 gap-12 pt-12 lg:grid-cols-2\"}).find_all(\"a\"):\n",
    "        article_link = get_article_link_hf(element)\n",
    "        try:\n",
    "            temp = {\"time_release\": element.find_all(\"span\")[-1].text,\n",
    "                    \"title\": element.h2.text.strip(),\n",
    "                    \"article_link\": article_link,\n",
    "                    \"images_links\": [f'https://huggingface.co{element.img[\"src\"]}']}\n",
    "            results.append(temp)\n",
    "        except: pass\n",
    "    return results\n",
    "\n",
    "# ------------- google ai\n",
    "\n",
    "def get_images_links_google_ai(soup):\n",
    "    results = []\n",
    "    for i in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            if \"https://blogger.googleusercontent.com/img\" in i[\"href\"]:\n",
    "                if i[\"href\"].rsplit(\".\", 1)[1] in [\"jpg\", \"png\"]:\n",
    "                    results.append(i[\"href\"])\n",
    "        except: pass\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_description_google_ai(soup) -> str:\n",
    "    elements = [i.text.strip() for i in soup.find_all(\"p\")]\n",
    "    element = sorted(elements, key=lambda x: len(x))\n",
    "    text = sent_tokenize(element[-2], language=\"english\")[:2]\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "\n",
    "def soup_to_embed_google_ai(soup):\n",
    "    results = []\n",
    "    for element in soup.find_all(\"div\", {\"class\": \"post\"}):\n",
    "        link = element.a[\"href\"]\n",
    "        element_soup = get_soup(link)\n",
    "        temp = {\"time_release\": element.span.text.strip(),\n",
    "                \"title\": element.a.text.strip(),\n",
    "                \"description\": get_description_google_ai(element_soup),\n",
    "                \"images_links\": get_images_links_google_ai(element_soup),\n",
    "                \"article_link\": link,\n",
    "                \"labels\": [j.text.strip() for j in element.find_all(\"a\", {\"class\": \"label\"})]}\n",
    "        results.append(temp)\n",
    "    return results\n",
    "\n",
    "# ------------- pytorch\n",
    "\n",
    "def soup_to_embed_pytorch(soup):\n",
    "    results = []\n",
    "    for element in soup.find(\"div\", {\"class\": \"row blog-vertical\"}).find_all(\"div\", {\"class\": \"col-md-4\"}):\n",
    "        temp = {\"time_release\": element.p.text,\n",
    "                \"title\": element.a.text,\n",
    "                \"description\": element.find_all(\"p\")[1].text.strip(),\n",
    "                \"article_link\": f'https://pytorch.org{element.a[\"href\"]}'}\n",
    "        results.append(temp)\n",
    "    return results\n",
    "\n",
    "# ------------- open ai\n",
    "\n",
    "def get_text_page_open_ai(link):\n",
    "    contents = requests.get(link).text\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    return [i.text for i in soup.find_all(\"p\") if i.text]\n",
    "\n",
    "\n",
    "def soup_to_embed_open_ai(soup):\n",
    "    results = []\n",
    "    for element in soup.find_all(\"div\", {\"class\": \"post-card-full medium-xsmall-copy\"}):\n",
    "        try:\n",
    "            article_link = f'https://openai.com{element.a[\"href\"]}'\n",
    "            temp = {\"time_release\": element.time.text,\n",
    "                    \"title\": element.h5.text,\n",
    "                    \"description\": get_text_page_open_ai(article_link)[0],\n",
    "                    \"article_link\": article_link,\n",
    "                    \"tag\": element.ul.a.text if element.ul else None}\n",
    "            results.append(temp)\n",
    "        except: pass\n",
    "    return results\n",
    "\n",
    "# ------------- nvidia-research\n",
    "\n",
    "def get_nvidia_research_publications(soup):\n",
    "    data = []\n",
    "    for element in soup.find_all(\"div\", {\"class\": \"views-row\"}):\n",
    "        try:\n",
    "            info = {\n",
    "                \"link\": f\"https://research.nvidia.com{element.a['href']}\",\n",
    "                \"authors\": element.find(\"div\", {\"class\": \"field-content\"}).text,\n",
    "                \"published-in-link\": element.find(\"div\", {\"class\": \"views-field views-field-field-published-in\"}).a[\"href\"],\n",
    "                \"published-in-conf\": element.find(\"div\", {\"class\": \"views-field views-field-field-published-in\"}).a.text,\n",
    "                \"time_release\": element.a['href'].split(\"_\")[0].split(\"/\")[-1] + \"-01\",\n",
    "                \"title\": element.a.text\n",
    "            }\n",
    "            data.append(info)\n",
    "        except: pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZlPDjefNPAeu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    idx = 0\n",
    "    for _ in range(len(data)):\n",
    "        try:\n",
    "            parser.parse(data[idx][\"time_release\"])\n",
    "            data[idx][\"time_release_new\"] = parser.parse(data[idx][\"time_release\"])\n",
    "            data[idx][\"time_release_new\"] = time.mktime(data[idx][\"time_release_new\"].timetuple())\n",
    "            idx += 1\n",
    "        except:\n",
    "            del data[idx]\n",
    "    data = sorted(data, key=lambda x: x[\"time_release_new\"], reverse=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_blog_posts():\n",
    "    yandex_research_posts = get_yandex_research_posts()\n",
    "    yandex_research_posts = prepare_data(yandex_research_posts)\n",
    "\n",
    "    openai_posts_soup = get_soup(\"https://openai.com/blog/\")\n",
    "    openai_posts = soup_to_embed_open_ai(openai_posts_soup)\n",
    "    openai_posts = prepare_data(openai_posts)\n",
    "\n",
    "    hf_posts_soup = get_soup(\"https://huggingface.co/blog/\")\n",
    "    hf_posts = soup_to_embed_hf(hf_posts_soup)\n",
    "    hf_posts = prepare_data(hf_posts)\n",
    "\n",
    "    pytorch_posts_soup = get_soup(\"https://pytorch.org/blog/\")\n",
    "    pytorch_posts = soup_to_embed_pytorch(pytorch_posts_soup)\n",
    "    pytorch_posts = prepare_data(pytorch_posts)\n",
    "\n",
    "    return yandex_research_posts, openai_posts, hf_posts, pytorch_posts\n",
    "\n",
    "\n",
    "def get_research_posts():\n",
    "    nvidia_research_posts_soup = get_soup(\"https://research.nvidia.com/research-area/machine-learning-artificial-intelligence\")\n",
    "    nvidia_research_posts = get_nvidia_research_publications(nvidia_research_posts_soup)\n",
    "    nvidia_research_posts = prepare_data(nvidia_research_posts)\n",
    "\n",
    "    google_ai_posts_soup = get_soup(\"https://responsible-ai-developers.googleblog.com/\")\n",
    "    google_ai_posts = soup_to_embed_google_ai(google_ai_posts_soup)\n",
    "    google_ai_posts = prepare_data(google_ai_posts)\n",
    "\n",
    "    yandex_research_posts = get_yandex_research_posts()\n",
    "    yandex_research_posts = prepare_data(yandex_research_posts)\n",
    "\n",
    "    deepmind_research_publications = get_deepmind_research_publications()\n",
    "    deepmind_research_publications = prepare_data(deepmind_research_publications)\n",
    "\n",
    "    google_research_publications = get_google_research_publications()\n",
    "    google_research_publications = prepare_data(google_research_publications)\n",
    "\n",
    "    return nvidia_research_posts, google_ai_posts, yandex_research_posts, deepmind_research_publications, google_research_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kGO6w8CBZsat",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_nvidia_post(nvidia_research_posts):\n",
    "    nvidia_research_posts = nvidia_research_posts[:5]\n",
    "    text = \"*Latest NVIDIA Research publications*:\\n\\n\"\n",
    "    for post in nvidia_research_posts:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     f\"–°onference/Venue: {post['published-in-conf']}\",\n",
    "                     f\"[Published URL]({post['published-in-link']}) | [Read now]({post['link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()\n",
    "\n",
    "\n",
    "def make_google_ai_post(google_ai_research_posts):\n",
    "    google_ai_research_posts = google_ai_research_posts[:3]\n",
    "    text = \"*Latest Google AI Research publications*:\\n\\n\"\n",
    "    for post in google_ai_research_posts:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     post[\"description\"],\n",
    "                     \" | \".join(post[\"labels\"]),\n",
    "                     f\"[Read now]({post['article_link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()\n",
    "\n",
    "\n",
    "def make_yandex_research_post(yandex_research_research_posts):\n",
    "    yandex_research_research_posts = [i for i in yandex_research_research_posts if i[\"type\"] != \"announcement\"][:3]\n",
    "    text = \"*Latest Yandex Research publications*:\\n\\n\"\n",
    "    for post in yandex_research_research_posts:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     post[\"seoDescription\"],\n",
    "                     post[\"type\"] + \" \" + \" | \".join(post[\"researchAreas\"]),\n",
    "                     f\"[Read now]({post['link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()\n",
    "\n",
    "\n",
    "def make_deepmind_research_post(deepmind_research_research_posts):\n",
    "    deepmind_research_research_posts = deepmind_research_research_posts[:3]\n",
    "    text = \"*Latest DeepMind Research publications*:\\n\\n\"\n",
    "    for post in deepmind_research_research_posts:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     post[\"description\"],\n",
    "                     f\"[Read now]({post['link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()\n",
    "\n",
    "\n",
    "def make_google_research_post(google_research_research_posts):\n",
    "    google_research_research_posts = google_research_research_posts[:3]\n",
    "    text = \"*Latest Google Research publications*:\\n\\n\"\n",
    "    for post in google_research_research_posts:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     post[\"abstract\"],\n",
    "                     f\"–°onference/Venue: {post['venue_text']}\",\n",
    "                     \" | \".join(post[\"tag_pks\"]),\n",
    "                     f\"[Read now]({post['link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KhunKMY7pZtf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_yandex_research_blog_post(yandex_research_research_blog):\n",
    "    yandex_research_research_blog = [i for i in yandex_research_research_blog if i[\"type\"] == \"announcement\"][:3]\n",
    "    text = \"*Latest Yandex Research Blog*:\\n\\n\"\n",
    "    for post in yandex_research_research_blog:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     post[\"seoDescription\"],\n",
    "                     post[\"type\"] + \" \" + \" | \".join(post[\"researchAreas\"]),\n",
    "                     f\"[Read now]({post['link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()\n",
    "\n",
    "\n",
    "def make_openai_blog_post(openai_research_blog):\n",
    "    openai_research_blog = openai_research_blog[:3]\n",
    "    text = \"*Latest Open AI Research Blog*:\\n\\n\"\n",
    "    for post in openai_research_blog:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     post[\"description\"],\n",
    "                     post[\"tag\"],\n",
    "                     f\"[Read now]({post['article_link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()\n",
    "\n",
    "\n",
    "def make_hf_blog_post(hf_research_blog):\n",
    "    hf_research_blog = hf_research_blog[:5]\n",
    "    text = \"*Latest HF Blog*:\\n\\n\"\n",
    "    for post in hf_research_blog:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     f\"[Read now]({post['article_link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()\n",
    "\n",
    "\n",
    "def make_pytorch_blog_post(pytorch_research_blog):\n",
    "    pytorch_research_blog = pytorch_research_blog[:5]\n",
    "    text = \"*Latest PyTorch Blog*:\\n\\n\"\n",
    "    for post in pytorch_research_blog:\n",
    "        paragraph = [f\"*{post['title']}*\",\n",
    "                     post[\"description\"],\n",
    "                     f\"[Read now]({post['article_link']})\"]\n",
    "        text += \"\\n\".join(paragraph) + \"\\n\\n\"\n",
    "    return text.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text: str):\n",
    "    tokens = tokenizer(text, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        emb = model(**tokens).pooler_output\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_search_answer(objects):\n",
    "    objects = objects[:5]\n",
    "    text = \"*Sematic Search Results*:\\n\\n\"\n",
    "    for post in objects:\n",
    "        paragraph = f\"[{post['title']}]\"\n",
    "        paragraph += f\"({post['article_link']})\" if \"article_link\" in post else f\"({post['link']})\"\n",
    "        text += paragraph + \"\\n\\n\"\n",
    "    return text.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "quc0ZyZwQTQn",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux/venv/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1165/4151564905.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  venue_text = BeautifulSoup(publication[\"venue_html\"], 'lxml').text\n"
     ]
    }
   ],
   "source": [
    "blog_posts = get_blog_posts()\n",
    "research_posts = get_research_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"blog_posts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blog_posts, f)\n",
    "    \n",
    "with open(\"research_posts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(research_posts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "dataset = blog_posts + research_posts\n",
    "dataset = [j for i in dataset for j in i]\n",
    "dataset = [(i, get_embedding(i[\"title\"])) for i in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bot = telebot.TeleBot(\"token\")\n",
    "\n",
    "blogs_dict = {\n",
    "    \"Yandex Research Blog\": make_yandex_research_blog_post(blog_posts[0]),\n",
    "    \"Open AI Blog\": make_openai_blog_post(blog_posts[1]),\n",
    "    \"Hugging Face Blog\": make_hf_blog_post(blog_posts[2]),\n",
    "    \"PyTorch Blog\": make_pytorch_blog_post(blog_posts[3])\n",
    "}\n",
    "\n",
    "publication_dict = {\n",
    "    \"NVIDIA AI Publications\": make_nvidia_post(research_posts[0]),\n",
    "    \"Google AI Publications\": make_google_ai_post(research_posts[1]),\n",
    "    \"Yandex Research Publications\": make_yandex_research_post(research_posts[2]),\n",
    "    \"DeepMind Research Publications\": make_deepmind_research_post(research_posts[3]),\n",
    "    \"Google Research Publications\": make_google_research_post(research_posts[4])\n",
    "}\n",
    "\n",
    "\n",
    "@bot.message_handler(commands=['start'])\n",
    "def start(message) -> None:\n",
    "    UsrInfo = bot.get_chat_member(message.from_user.id, message.from_user.id).user\n",
    "    \n",
    "    markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
    "    button_create = types.KeyboardButton(\"üëã –°–æ–±—Ä–∞—Ç—å —Å–≤–µ–∂—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\")\n",
    "    markup.add(button_create)\n",
    "    \n",
    "    bot.send_message(message.chat.id, f'üíª –ü—Ä–∏–≤–µ—Ç, {UsrInfo.first_name}! –Ø *–±–æ—Ç-–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä* –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –≤ *Computer Science* & *Data Science*. '\n",
    "                                      '–ú–æ–≥—É –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏ –≥–æ—Ç–æ–≤—è—â–∏–µ—Å—è –∞–Ω–æ–Ω—Å—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π üöÄ\\n\\n'\n",
    "                                      '–î–∞–≤–∞–π –ø—Ä–æ–≤–µ—Ä–∏–º –º–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª ü§ü',\n",
    "                     reply_markup=markup, parse_mode='Markdown')\n",
    "    \n",
    "\n",
    "@bot.message_handler(commands=['search'])\n",
    "def search(message) -> None:\n",
    "    message.text = message.text[9:]\n",
    "    if message.text:\n",
    "        emb = get_embedding(message.text)\n",
    "        response = [(i[0], torch.nn.functional.cosine_similarity(i[1], emb).cpu().detach().numpy()[0]) for i in dataset]\n",
    "        results = sorted(response, key=lambda x: x[1], reverse=True)\n",
    "        results = [i[0] for i in results]\n",
    "        answer = make_search_answer(results)\n",
    "        print(answer)\n",
    "\n",
    "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
    "        btn = types.KeyboardButton(\"–ù–∞–∑–∞–¥\")\n",
    "        markup.add(btn)\n",
    "\n",
    "        bot.send_message(message.chat.id, answer, reply_markup=markup, parse_mode='Markdown')\n",
    "    else:\n",
    "        bot.send_message(message.chat.id, \"–ó–∞–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π\")\n",
    "        message.text = \"–ü–æ–∏—Å–∫\"\n",
    "        func(message)\n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def func(message):\n",
    "    if message.text == \"üëã –°–æ–±—Ä–∞—Ç—å —Å–≤–µ–∂—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\":\n",
    "        bot.send_message(message.chat.id, \"–°–æ–±–∏—Ä–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ë–æ–ª–µ–µ 7 —Ç—ã—Å—è—á –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤...|\")\n",
    "        time.sleep(0.1)\n",
    "        bot.send_message(message.chat.id, \"–ü—Ä–∏–≤–æ–∂—É –≤ —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–π –≤–∏–¥...\")\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
    "        btn1 = types.KeyboardButton(\"–ë–ª–æ–≥–∏\")\n",
    "        btn2 = types.KeyboardButton(\"–ü—É–±–ª–∏–∫–∞—Ü–∏–∏\")\n",
    "        btn3 = types.KeyboardButton(\"–ü–æ–∏—Å–∫\")\n",
    "        markup.add(btn1, btn2, btn3)\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"–ì–æ—Ç–æ–≤–æ! –ù—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é\\n\"\n",
    "                                          \"*–ë–ª–æ–≥–∏* - –∞–Ω–æ–Ω—Å—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π, –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\\n\"\n",
    "                                          \"*–ü—É–±–ª–∏–∫–∞—Ü–∏–∏* - –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –∂—É—Ä–Ω–∞–ª–∞—Ö –∏ –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è—Ö\\n\"\n",
    "                                          \"*–ü–æ–∏—Å–∫* - –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∏—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –±–∞–∑–µ\",\n",
    "                         parse_mode='Markdown', reply_markup=markup)\n",
    "    \n",
    "    # =======\n",
    "    elif message.text == \"–ë–ª–æ–≥–∏\":\n",
    "        bot.send_message(message.chat.id, \"–°—É–ø–µ—Ä üî•\")\n",
    "        \n",
    "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
    "        btns = [types.KeyboardButton(i) for i in blogs_dict.keys()]\n",
    "        btns += [types.KeyboardButton(\"–ù–∞–∑–∞–¥\")]\n",
    "        markup.add(*btns)\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"–í–æ—Ç, —á—Ç–æ –º–Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å:\", reply_markup=markup)\n",
    "        \n",
    "    elif message.text.endswith(\"Blog\"):\n",
    "        try:\n",
    "            bot.send_message(message.chat.id, blogs_dict[message.text], parse_mode='Markdown')\n",
    "        except:\n",
    "            bot.send_message(message.chat.id, blogs_dict[message.text])\n",
    "    \n",
    "    # =======\n",
    "    elif message.text == \"–ü—É–±–ª–∏–∫–∞—Ü–∏–∏\":\n",
    "        bot.send_message(message.chat.id, \"–°—É–ø–µ—Ä üî•\")\n",
    "        \n",
    "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
    "        btns = [types.KeyboardButton(i) for i in publication_dict.keys()]\n",
    "        btns += [types.KeyboardButton(\"–ù–∞–∑–∞–¥\")]\n",
    "        markup.add(*btns)\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"–í–æ—Ç, —á—Ç–æ –º–Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å:\", reply_markup=markup)\n",
    "        \n",
    "    elif message.text.endswith(\"Publications\"):\n",
    "        try:\n",
    "            bot.send_message(message.chat.id, publication_dict[message.text], parse_mode='Markdown')\n",
    "        except:\n",
    "            bot.send_message(message.chat.id, publication_dict[message.text])\n",
    "            \n",
    "     # =======\n",
    "    \n",
    "    elif message.text == \"–ù–∞–∑–∞–¥\":\n",
    "        message.text = \"üëã –°–æ–±—Ä–∞—Ç—å —Å–≤–µ–∂—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\"\n",
    "        func(message)\n",
    "        \n",
    "    # -------\n",
    "    \n",
    "    elif message.text == \"–ü–æ–∏—Å–∫\":\n",
    "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
    "        btn = types.KeyboardButton(\"/search transformers in text\")\n",
    "        markup.add(btn)\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"–í–æ—Å–ø–æ–ª—å–∑—É–π—Å—è –∫–æ–º–∞–Ω–¥–æ–π –¥–ª—è –ø–æ–∏—Å–∫–∞: /search {—Ç–µ–∫—Å—Ç —Å—é–¥–∞}\", reply_markup=markup)\n",
    "    \n",
    "    else:\n",
    "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
    "        btn = types.KeyboardButton(\"üëã –°–æ–±—Ä–∞—Ç—å —Å–≤–µ–∂—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\")\n",
    "        markup.add(btn)\n",
    "        \n",
    "        bot.send_message(message.chat.id, \"–Ø —Ç–µ–±—è –Ω–µ –ø–æ–Ω—è–ª, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π –Ω–∞—á–Ω–µ–º —Å–Ω–∞—á–∞–ª–∞\", reply_markup=markup)\n",
    "        \n",
    "    \n",
    "bot.polling(none_stop=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
